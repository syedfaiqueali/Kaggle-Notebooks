{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install ultralytics -q","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-05T10:00:53.128567Z","iopub.execute_input":"2024-11-05T10:00:53.129278Z","iopub.status.idle":"2024-11-05T10:01:09.774379Z","shell.execute_reply.started":"2024-11-05T10:00:53.129217Z","shell.execute_reply":"2024-11-05T10:01:09.772411Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"# Common\nimport time\nimport os\nimport csv\nimport numpy as np\n\n# Data Visualization\nimport plotly.express as px\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n\n# Image Processing\nimport cv2\nfrom PIL import Image\n\n# Yolo\nfrom ultralytics import YOLO","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-05T10:01:32.169954Z","iopub.execute_input":"2024-11-05T10:01:32.171054Z","iopub.status.idle":"2024-11-05T10:01:36.491064Z","shell.execute_reply.started":"2024-11-05T10:01:32.170998Z","shell.execute_reply":"2024-11-05T10:01:36.489733Z"}},"outputs":[{"name":"stdout","text":"Creating new Ultralytics Settings v0.0.6 file âœ… \nView Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\nUpdate Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# Constants","metadata":{}},{"cell_type":"code","source":"BASE_PATH = '/kaggle/input/horse-riding-jumping-walking'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-05T10:01:45.147863Z","iopub.execute_input":"2024-11-05T10:01:45.148631Z","iopub.status.idle":"2024-11-05T10:01:45.156470Z","shell.execute_reply.started":"2024-11-05T10:01:45.148584Z","shell.execute_reply":"2024-11-05T10:01:45.154513Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"# Util","metadata":{}},{"cell_type":"code","source":"def get_image_paths(directory, valid_extensions=(\".jpg\", \".jpeg\", \".png\", \".bmp\", \".gif\", \".mp4\")):\n    image_paths = []\n    \n    # Iterate over all files in the directory\n    for root, dirs, files in os.walk(directory):\n        for file in files:\n            # Check if file is an image based on its extension\n            if file.lower().endswith(valid_extensions):\n                # Append the full path of the image file to the list\n                image_paths.append(os.path.join(root, file))\n    \n    return image_paths\n\n\ndef plot_image_grid(images, images_per_row=5, total_images=50):\n    # Ensure we don't exceed the available images or the limit of 50\n    total_images = min(total_images, len(images))\n    \n    # Calculate the number of rows needed\n    num_rows = (total_images + images_per_row - 1) // images_per_row\n\n    # Create a figure with a grid of subplots\n    fig, axs = plt.subplots(num_rows, images_per_row, figsize=(15, 3 * num_rows))\n\n    # Flatten the axes array in case of more than one row\n    axs = axs.ravel()\n\n    for i in range(total_images):\n        # Read and display the image\n        img = images[i]\n        axs[i].imshow(img)\n        axs[i].axis('off')  # Turn off the axis for clean presentation\n\n    # Turn off any remaining unused subplots (if total_images < images_per_row * num_rows)\n    for j in range(total_images, len(axs)):\n        axs[j].axis('off')\n\n    # Show the grid of images\n    plt.tight_layout()\n    plt.show()\n\n\ndef extract_frames_from_video(video_path: str) -> list:\n    frames = []\n    \n    # Open the video file\n    video_capture = cv2.VideoCapture(video_path)\n    \n    # Check if the video opened successfully\n    if not video_capture.isOpened():\n        print(\"Error: Could not open video.\")\n        return frames\n    \n    # Loop through video frames\n    while True:\n        # Read frame-by-frame\n        ret, frame = video_capture.read()\n        \n        # Break the loop if no frame is captured\n        if not ret:\n            break\n        \n        # Append the frame (as a numpy array) to the list\n        # frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n        frames.append(frame)\n    \n    # Release the video capture object\n    video_capture.release()\n    \n    return frames\n\n    \ndef plt_img(image, cmap=\"gray\"):\n    \"\"\"Display an image using matplotlib\"\"\"\n    plt.imshow(image, cmap)\n    plt.axis('on')\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-05T10:09:28.844977Z","iopub.execute_input":"2024-11-05T10:09:28.845461Z","iopub.status.idle":"2024-11-05T10:09:28.859305Z","shell.execute_reply.started":"2024-11-05T10:09:28.845415Z","shell.execute_reply":"2024-11-05T10:09:28.857815Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"def load_yolo_model():\n    # Load the YOLOv5 model\n    model = YOLO('yolov5su.pt') # yolov5s.pt, yolov5su.pt\n    return model\n\ndef detect_horses(model_infer, frame, confidence_threshold=0.8):\n    detected_horses = []\n    \n    # Perform inference on the frame\n    results = model_infer(frame)\n\n    # Iterate through detected results\n    for result in results:\n        for box in result.boxes:\n            # Retrieve class name, confidence, and bounding box coordinates\n            class_id = int(box.cls)\n            class_name = result.names[class_id]  # Get the class name\n            confidence = box.conf.item()  # Confidence score\n\n            if (class_name == \"horse\") and (confidence >= confidence_threshold):\n                bbox = box.xyxy[0].tolist()  # Bounding box [x_min, y_min, x_max, y_max]\n                detected_horses.append((class_name, confidence, bbox))\n                # print(f\"Class: {class_name}, Confidence: {confidence:.2f}, BBox: {bbox}\")\n\n    return detected_horses","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-05T11:04:39.853933Z","iopub.execute_input":"2024-11-05T11:04:39.854901Z","iopub.status.idle":"2024-11-05T11:04:39.862846Z","shell.execute_reply.started":"2024-11-05T11:04:39.854831Z","shell.execute_reply":"2024-11-05T11:04:39.861415Z"}},"outputs":[],"execution_count":41},{"cell_type":"markdown","source":"# Step1: Read Video Paths","metadata":{}},{"cell_type":"code","source":"video_paths = get_image_paths(BASE_PATH)\nvideo_paths","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-05T11:04:09.815257Z","iopub.execute_input":"2024-11-05T11:04:09.815764Z","iopub.status.idle":"2024-11-05T11:04:09.834004Z","shell.execute_reply.started":"2024-11-05T11:04:09.815724Z","shell.execute_reply":"2024-11-05T11:04:09.832793Z"}},"outputs":[{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"['/kaggle/input/horse-riding-jumping-walking/3.mp4',\n '/kaggle/input/horse-riding-jumping-walking/1.mp4',\n '/kaggle/input/horse-riding-jumping-walking/4.mp4',\n '/kaggle/input/horse-riding-jumping-walking/2.mp4']"},"metadata":{}}],"execution_count":37},{"cell_type":"code","source":"video_frames = extract_frames_from_video(video_paths[-1])\nlen(video_frames)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-05T11:04:10.204922Z","iopub.execute_input":"2024-11-05T11:04:10.205686Z","iopub.status.idle":"2024-11-05T11:04:19.021878Z","shell.execute_reply.started":"2024-11-05T11:04:10.205613Z","shell.execute_reply":"2024-11-05T11:04:19.020672Z"}},"outputs":[{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"5029"},"metadata":{}}],"execution_count":38},{"cell_type":"code","source":"yolov5_model = load_yolo_model()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-05T11:04:19.023692Z","iopub.execute_input":"2024-11-05T11:04:19.024103Z","iopub.status.idle":"2024-11-05T11:04:19.103780Z","shell.execute_reply.started":"2024-11-05T11:04:19.024063Z","shell.execute_reply":"2024-11-05T11:04:19.102515Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"test_frame = video_frames[99]\ndetected_horses = detect_horses(frame=test_frame, model_infer=yolov5_model)\ndetected_horses","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-05T11:04:51.637631Z","iopub.execute_input":"2024-11-05T11:04:51.638084Z","iopub.status.idle":"2024-11-05T11:04:51.851725Z","shell.execute_reply.started":"2024-11-05T11:04:51.638042Z","shell.execute_reply":"2024-11-05T11:04:51.850383Z"}},"outputs":[{"name":"stdout","text":"\n0: 384x640 4 persons, 1 horse, 1 potted plant, 195.5ms\nSpeed: 2.2ms preprocess, 195.5ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n","output_type":"stream"},{"execution_count":42,"output_type":"execute_result","data":{"text/plain":"[('horse',\n  0.8800753355026245,\n  [495.6009826660156, 105.20941162109375, 585.3511962890625, 219.91455078125])]"},"metadata":{}}],"execution_count":42},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}