{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7962198,"sourceType":"datasetVersion","datasetId":4619135}],"dockerImageVersionId":30673,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install openai==1.14.2","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-03-28T08:43:43.327858Z","iopub.execute_input":"2024-03-28T08:43:43.328232Z","iopub.status.idle":"2024-03-28T08:44:00.525814Z","shell.execute_reply.started":"2024-03-28T08:43:43.328204Z","shell.execute_reply":"2024-03-28T08:44:00.524634Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"import io\nimport re\nimport os\nimport json\nimport base64\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n\nfrom openai import OpenAI","metadata":{"execution":{"iopub.status.busy":"2024-03-28T08:44:00.527636Z","iopub.execute_input":"2024-03-28T08:44:00.528043Z","iopub.status.idle":"2024-03-28T08:44:03.158818Z","shell.execute_reply.started":"2024-03-28T08:44:00.528010Z","shell.execute_reply":"2024-03-28T08:44:03.157589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Constant","metadata":{}},{"cell_type":"code","source":"# Env variables\nos.environ[\"OPEN_API_KEY\"] = \"API-KEY\"\n\n# File Paths\nCLEANED_DATA_PATH = '/kaggle/input/rta-dubai/sample_clean_application_data_2.xlsx'\nUNCLEANED_DATA_PATH = '/kaggle/input/rta-dubai/sample_unclean_application_data_v2.xlsx'\nMETADATA_PATH = '/kaggle/input/rta-dubai/Application dataset.json'\n\nOPEN_AI_CLIENT = OpenAI(api_key=os.getenv(\"OPEN_API_KEY\") ,organization='org-2EugvoZZidKLd5DkFH3dGTIA')","metadata":{"execution":{"iopub.status.busy":"2024-03-28T08:44:03.160564Z","iopub.execute_input":"2024-03-28T08:44:03.161047Z","iopub.status.idle":"2024-03-28T08:44:03.184309Z","shell.execute_reply.started":"2024-03-28T08:44:03.161017Z","shell.execute_reply":"2024-03-28T08:44:03.182863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Models","metadata":{}},{"cell_type":"code","source":"class MetaData:\n    def __init__(self, data):\n        self.__dict__ = data","metadata":{"execution":{"iopub.status.busy":"2024-03-28T04:31:59.078732Z","iopub.execute_input":"2024-03-28T04:31:59.079045Z","iopub.status.idle":"2024-03-28T04:31:59.084121Z","shell.execute_reply.started":"2024-03-28T04:31:59.079018Z","shell.execute_reply":"2024-03-28T04:31:59.082684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Helper Function","metadata":{}},{"cell_type":"code","source":"def load_json_and_map_to_class(json_path):\n    try:\n        with open(json_path, 'r') as json_file:\n            data = json.load(json_file)\n            return MetaData(data)\n    except json.JSONDecodeError:\n        print(f\"Error: Unable to decode JSON from file '{file_path}'. File may be empty or not in valid JSON format.\")\n        return None\n\n\ndef extract_column_attributes_for_column_name_mapping(metadata_json):\n    columns_meta_data = []\n    for column in metadata_json.attributes:\n        columns_meta_data.append({\n            \"attribute_name\": column['attribute_name'],\n            \"description\": column['description'],\n            \"description\": column['description']\n        })\n    return json.dumps(columns_meta_data)\n\n\ndef convert_dataframe_to_html(dataframe):\n    html_string = dataframe.to_html()\n    return html_string.replace('\\n', '')\n\n\ndef extract_python_code(sample_string):\n    # Define a regular expression pattern to match Python code blocks\n    pattern = r'```python\\n(.*?)```'\n\n    # Find all matches of the pattern in the sample string\n    matches = re.findall(pattern, sample_string, re.DOTALL)\n\n    # Return the Python code blocks found\n    return matches\n\n\ndef convert_column_types_and_map_values_wrt_metadata(metadata_json, dataframe):\n    for column in metadata_json.attributes:\n        if column['attribute_name'] in dataframe.columns:\n            if column['data_type'].lower() == \"string\":\n                dataframe[column[\"attribute_name\"]] = dataframe[column[\"attribute_name\"]].astype(str)\n            elif column['data_type'].lower() == \"enum\":\n                # Check if enum value transformation required\n                if column['is_transformation_required']:\n                    dataframe[column[\"attribute_name\"]] = dataframe[column[\"attribute_name\"]].astype(str).str.lower().map(column['values'])\n                dataframe[column[\"attribute_name\"]] = dataframe[column[\"attribute_name\"]].astype('category')\n            elif column['data_type'].lower() == \"date\":\n                dataframe[column[\"attribute_name\"]] = dataframe[column[\"attribute_name\"]].astype('datetime64[ns]')\n                \n    return dataframe\n\n\ndef check_if_any_new_column_formation_is_required(dataframe, metadata_json):\n    new_columns_data = []\n    for column in metadata_json.attributes:\n        if (column['attribute_name'] not in dataframe) and column['constraints']:\n            column_name = column['attribute_name']\n            column_type = column['data_type']\n            column_desc = column['constraints'][0]\n            new_columns_data.append((column_name, column_type, column_desc))\n    return new_columns_data\n\n\ndef extract_df_column_names(dataframe):\n    return \", \".join(dataframe.columns.tolist())\n\n\ndef extract_df_column_names_with_datatypes(dataframe):\n    columns_data_types = {col: str(dtype) for col, dtype in dataframe.dtypes.items()}\n    json_data = json.dumps(columns_data_types)\n    return json_data\n\n\ndef execute_python_script(python_script):\n    slice_script = python_script[9:-3]\n    exec(slice_script)\n    \n\ndef read_saved_dataframe_output():\n    with open('output.txt', 'r') as file:\n        html_content = file.read()\n\n    return html_content\n\n\ndef read_saved_plot_output():\n    with open('output.png', 'rb') as img_file:\n        img_base64 = base64.b64encode(img_file.read()).decode('utf-8')\n    return img_base64\n\n\ndef read_saved_dataframe_new_col_output():\n    with open('dataframe.txt', 'r') as file:\n        html_content = file.read()\n\n    # Parse HTML content and convert to DataFrame\n    # [TODO] Later remove this, as only for notebook perspective\n    return pd.read_html(html_content)[0]\n\n\ndef contains_visual_keywords(text):\n    visual_keywords = ['plot', 'graph', 'chart', 'visualization']\n    pattern = re.compile(r'\\b(?:' + '|'.join(visual_keywords) + r')\\b', flags=re.IGNORECASE)\n    return True if re.search(pattern, text) else False\n\n\ndef get_html_tag_for_base64_image(img_base64):\n    return f'<img src=\"data:image/png;base64,{img_base64}\" style=\"max-width: 100%; max-height: 100%;\" alt=\"output\">'\n\n\ndef get_dataframe_dtypes_metadata(dataframe):\n    column_types = {col: str(dataframe[col].dtype) for col in dataframe.columns}\n    return json.dumps(column_types)\n\n\ndef load_finalised_csv_dtypes_metadata():\n    with open('finalised_metadata.json', 'r') as json_file:\n        return json.load(json_file)\n\n    \ndef show_saved_output_img():\n    img = mpimg.imread('output.png')\n    img_height, img_width, _ = img.shape\n    aspect_ratio = img_width / img_height\n    fig_width = 10  # Set a default width\n    fig_height = fig_width / aspect_ratio\n    fig = plt.figure(figsize=(fig_width, fig_height))\n    plt.imshow(img, interpolation='nearest')\n    plt.axis('off')\n    plt.tight_layout()\n    # return fig  # Return the figure object","metadata":{"execution":{"iopub.status.busy":"2024-03-28T04:57:55.475329Z","iopub.execute_input":"2024-03-28T04:57:55.475969Z","iopub.status.idle":"2024-03-28T04:57:55.496500Z","shell.execute_reply.started":"2024-03-28T04:57:55.475936Z","shell.execute_reply":"2024-03-28T04:57:55.495680Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# OpenAI Utils","metadata":{}},{"cell_type":"code","source":"class RTA_Bot:\n    _instance = None\n\n    def __new__(cls, *args, **kwargs):\n        if not cls._instance:\n            cls._instance = super().__new__(cls, *args, **kwargs)\n        return cls._instance\n\n    @staticmethod\n    def ask(prompt):\n        return OPEN_AI_CLIENT.chat.completions.create(\n            model=\"gpt-3.5-turbo\",\n            messages=[\n            {\n                \"role\": \"user\",\n                \"content\": prompt,\n            }],\n        ).choices[0].message.content","metadata":{"execution":{"iopub.status.busy":"2024-03-28T04:57:56.430891Z","iopub.execute_input":"2024-03-28T04:57:56.431297Z","iopub.status.idle":"2024-03-28T04:57:56.437354Z","shell.execute_reply.started":"2024-03-28T04:57:56.431261Z","shell.execute_reply":"2024-03-28T04:57:56.436312Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def map_column_names_wrt_meta_data(dataset_column_names, metadata_json):\n    prompt = f\"\"\"\nRole: Act as a data analyst whose job is to help in transforming the dataset.\nTask: Consider below defined \"dataset_column_names\". Look up the \"columns_meta_data_json\" and pick relevant column attribute_name w.r.t to the attribute description and synonyms context only where required. Output should be a json with key as 'dataset_column_names' and value as attribute_name picked from columns_meta_data_json\n\ndataset_column_names: {dataset_column_names}\n\ncolumns_meta_data_json: \n{metadata_json}\n\nOutput: Return output in JSON Format defined under sample output.\n\nSample Output: {{\"Column1 Old Name\": \"Column1 New Name\", \"Column2 Old Name\": \"Column2 New Name\", ...}}\n\"\"\"\n    response = RTA_Bot.ask(prompt)\n    response = json.loads(response)\n    response = {key: value for key, value in response.items() if value != \"\" and value is not None}\n    return response\n\n\ndef ask_rta_bot(question):\n    column_names = load_finalised_csv_dtypes_metadata()\n    \n    if contains_visual_keywords(question):\n        execute_visual_prompt(question,column_names)\n    else:\n        output_dataframe = execute_dataframe_prompt(question, column_names)\n        return output_dataframe\n\n\ndef execute_dataframe_prompt(question, column_names):\n    prompt = f\"\"\" \nRole: Act as a data analyst whose job is to extract insights from the dataset based on the users asked questions.\n\nTask: Write a valid Python script for the below-mentioned \"user_question\". Load \"finalised.csv\" only for querying the data.\n\nuser_question: {question}\n\n\"finalised.csv\" pandas columns names and their types: {column_names}\n\noutput: The output should only be a valid python script\n\nScript steps:\n1. Read .csv file named 'finalised.csv'\n2. Convert the data types of the above read 'finalised.csv' dataframe as per the datatypes defined above\n3. Code for the task\n4. Form a pandas dataframe of the final output\n5. Then convert output dataframe into html string\n6. Save the output into a file named 'output.txt'\n\"\"\"\n    print(prompt)\n    try:\n        # Generate response\n        python_script = RTA_Bot.ask(prompt)\n        print(python_script)\n\n        # Execute script\n        execute_python_script(python_script)\n\n        # Read output dataframe as html table tag\n        output = read_saved_dataframe_output()\n\n        # Remove file\n        # os.remove('output.txt')\n\n        # Return output\n        return output\n    except Exception as e:\n        print(\"I apologize, but I'm currently unable to fulfill your request. Could you please try again or provide me more details to assist me in better understanding your request?\")\n        print(e)\n        return None\n    \n    \ndef execute_visual_prompt(question, column_names):\n    prompt = f\"\"\" \nRole: Act as a data analyst whose job is extract insights from the dataset based on the users asked question.\n\nTask: Write a valid python script for the below mentioned \"user_question\". Load \"finalised.csv\" only for querying the data.\n\nuser_question: {question}\n\n\"finalised.csv\"  pandas columns names and their types: {column_names}\n\noutput: The output should only be a valid python script\n\nScript steps:\n1. Read .csv file named 'finalised.csv'\n2. Convert the data types of the above read 'finalised.csv' dataframe as per the datatypes defined above\n3. Code for the user's asked question.\n4. Form a plot as the final output, no need to write plt.show() code.\n5. Save the output plot as a .png file named 'output.png'\n\"\"\"\n    print(prompt)\n    try:\n        # Generate response\n        python_script = RTA_Bot.ask(prompt)\n        print(python_script)\n\n        # Execute script\n        execute_python_script(python_script)\n\n        # Read output image as html image tag\n        output = get_html_tag_for_base64_image(read_saved_plot_output())\n        \n        # [REMOVE]\n        # show_saved_output_img()\n        \n        # Remove file\n        # os.remove('output.png')\n\n        # Return output\n        return output\n        \n    except Exception as e:\n        print(\"I apologize, but I'm currently unable to fulfill your request. Could you please try again or provide me more details to assist me in better understanding your request?\")\n        print(e)\n        return None\n\ndef execute_create_new_column_prompt(new_column, existing_df_columns_metadata):\n    new_column_name = new_column[0]\n    new_column_type = new_column[1]\n    new_column_desc = new_column[2]\n    \n    prompt = f\"\"\"\nRole: Act as a data analyst whose job is to create new columns from the below defined 'finalised.csv' data columns names and their data types.\n\nTask: Write a valid python script for the below mentioned \"user_question\". Use 'finalised.csv' only for querying the data.\n\nuser_question: Create a new column named \"{new_column_name}\" with type {new_column_type} and constraints as {new_column_desc}.\n\nexisting pandas columns names and their types: {existing_df_columns_metadata}\n\nScript steps:\n1. Read .csv file named 'finalised.csv'\n2. Convert the data types of the above read 'finalised.csv' dataframe as per the datatypes defined above\n3. Code for the task.\n4. Form a pandas dataframe of the final output with newly added column and its supporting column.\n5. Then convert output dataframe into html string\n6. Save the output into a file named 'dataframe.txt'\n\n    \"\"\"\n    try:\n        # Generate response\n        python_script = RTA_Bot.ask(prompt)\n        print(python_script)\n        \n        # Execute script\n        execute_python_script(python_script)\n\n        # Read output dataframe as html table tag\n        output = read_saved_dataframe_new_col_output()\n\n        # Remove file\n        # os.remove('dataframe.txt')\n\n        # Return output\n        return output\n    except Exception as e:\n        print(\"--------Error in new column formation--------\")\n        print(prompt)\n        print(e)\n        return None\n","metadata":{"execution":{"iopub.status.busy":"2024-03-28T05:13:39.201792Z","iopub.execute_input":"2024-03-28T05:13:39.202179Z","iopub.status.idle":"2024-03-28T05:13:39.217313Z","shell.execute_reply.started":"2024-03-28T05:13:39.202152Z","shell.execute_reply":"2024-03-28T05:13:39.216211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Pipeline\n\n1. Extract **unclean dataset column names** and **attributes objects** from the meta-data.\n2. Use Open-AI to map **column new names** w.r.t to the provided meta-data.\n3. Change columns names.\n4. Transform columns types and their values w.r.t to the **metadata**\n5. Check if new columns formation is left w.r.t to the **metadata**\n6. Talk with the transformed dataset\n\n\n# Code","metadata":{}},{"cell_type":"code","source":"def execute_pipline():\n    # -------------------------------------------------------\n    # 1. Read the un-cleaned dataset and metadata json\n    # -------------------------------------------------------\n    df_uncleaned = pd.read_excel(UNCLEANED_DATA_PATH)\n    metadata_json = load_json_and_map_to_class(METADATA_PATH)\n    \n    \n    # -------------------------------------------------------\n    # 2. Column name mapping w.r.t metadata json\n    # -------------------------------------------------------\n    # Extract old dataset column names as list of string\n    old_column_names = df_uncleaned.columns.tolist()\n\n    # Find column names w.r.t metadata whose names are already correct\n    metadata_attributes = {attr['attribute_name'] for attr in metadata_json.attributes}\n    remove_column_names = [col_name for col_name in old_column_names if col_name in metadata_attributes]\n    \n    # Filter column and their metadata whose names are not correct\n    remaining_col_names = [col_name for col_name in old_column_names if col_name not in remove_column_names]\n    remaining_col_meta_data = []\n    for attr in metadata_json.attributes:\n        if attr['attribute_name'] not in remove_column_names:\n            remaining_col_meta_data.append({\n                \"attribute_name\": attr['attribute_name'],\n                \"description\": attr['description'],\n                \"synonyms\": attr['synonyms']\n            })\n    \n    remaining_col_metadata = json.dumps(remaining_col_meta_data)\n\n    # Use Open-Ai to map new column names w.r.t to context\n    uncleaned_column_new_names = map_column_names_wrt_meta_data(remaining_col_names, remaining_col_metadata)\n\n    # Rename columns\n    df_uncleaned.rename(columns=uncleaned_column_new_names, inplace=True)\n    \n    # -------------------------------------------------------\n    # 3. Convert column type w.r.t metadata json\n    # -------------------------------------------------------\n    df_uncleaned = convert_column_types_and_map_values_wrt_metadata(metadata_json, df_uncleaned)\n    df_uncleaned.to_csv('finalised.csv', index=False)\n    with open('finalised_metadata.json', \"w\") as json_file:\n        json.dump(get_dataframe_dtypes_metadata(df_uncleaned), json_file)\n    \n\n    # -------------------------------------------------------\n    # 4. Add new column if any missing w.r.t metadatajson\n    # -------------------------------------------------------\n    # Extract new columns data\n#     new_columns = check_if_any_new_column_formation_is_required(df_uncleaned, metadata_json)\n    \n#     # Create new columns\n#     for new_column in new_columns:\n#         new_column_name = new_column[0]\n#         new_column_df = execute_create_new_column_prompt(\n#             new_column,\n#             extract_df_column_names_with_datatypes(df_uncleaned)\n#         )\n#         # Concatenates column to the main dataset & remove duplicates column wise\n#         if new_column_name in new_column_df.columns:\n#             df_uncleaned = pd.concat([df_uncleaned, new_column_df], axis=1)\n#             df_uncleaned = df_uncleaned.loc[:,~df_uncleaned.columns.duplicated()]\n#             df_uncleaned = df_uncleaned.loc[:, ~df_uncleaned.columns.str.contains('^Unnamed')]\n    \n#     # -------------------------------------------------------\n#     # 5. Save modified dataset\n#     # -------------------------------------------------------\n#     df_uncleaned.to_csv('finalised.csv', index=False)\n#     with open('finalised_metadata.json', \"w\") as json_file:\n#         json.dump(get_dataframe_dtypes_metadata(df_uncleaned), json_file)\n","metadata":{"execution":{"iopub.status.busy":"2024-03-28T05:13:39.732751Z","iopub.execute_input":"2024-03-28T05:13:39.733359Z","iopub.status.idle":"2024-03-28T05:13:39.744852Z","shell.execute_reply.started":"2024-03-28T05:13:39.733323Z","shell.execute_reply":"2024-03-28T05:13:39.743574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Execution","metadata":{}},{"cell_type":"code","source":"%%time\n\n# try:\n#     execute_pipline()\n#     print(\"Execution successfull\")\n# except Exception as e:\n#     print(\"Execution failed: \", e)\n\nexecute_pipline()","metadata":{"execution":{"iopub.status.busy":"2024-03-28T05:13:40.354319Z","iopub.execute_input":"2024-03-28T05:13:40.354709Z","iopub.status.idle":"2024-03-28T05:13:43.524167Z","shell.execute_reply.started":"2024-03-28T05:13:40.354677Z","shell.execute_reply":"2024-03-28T05:13:43.523329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('finalised.csv')\ndf.head(2)","metadata":{"execution":{"iopub.status.busy":"2024-03-27T16:28:01.127254Z","iopub.execute_input":"2024-03-27T16:28:01.128001Z","iopub.status.idle":"2024-03-27T16:28:01.259950Z","shell.execute_reply.started":"2024-03-27T16:28:01.127940Z","shell.execute_reply":"2024-03-27T16:28:01.258354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open('finalised_metadata.json', 'r') as json_file:\n    data = json.load(json_file)\n    print(data)","metadata":{"execution":{"iopub.status.busy":"2024-03-23T16:26:18.368469Z","iopub.execute_input":"2024-03-23T16:26:18.369479Z","iopub.status.idle":"2024-03-23T16:26:18.375205Z","shell.execute_reply.started":"2024-03-23T16:26:18.369441Z","shell.execute_reply":"2024-03-23T16:26:18.373949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ask_rta_bot('What is the User Community percentage w.r.t to its unique values')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ask_rta_bot('Check for data quality issues which should include missing values, unique values, duplicate values and their data types')","metadata":{"execution":{"iopub.status.busy":"2024-03-23T16:49:13.006740Z","iopub.execute_input":"2024-03-23T16:49:13.007132Z","iopub.status.idle":"2024-03-23T16:49:19.476019Z","shell.execute_reply.started":"2024-03-23T16:49:13.007089Z","shell.execute_reply":"2024-03-23T16:49:19.474833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ask_rta_bot('Plot Pie chart of Status', df_uncleaned)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ask_rta_bot('Plot Bar graph of Status')","metadata":{"execution":{"iopub.status.busy":"2024-03-23T16:50:25.508593Z","iopub.execute_input":"2024-03-23T16:50:25.509373Z","iopub.status.idle":"2024-03-23T16:50:30.967173Z","shell.execute_reply.started":"2024-03-23T16:50:25.509339Z","shell.execute_reply":"2024-03-23T16:50:30.965949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"json_data = {\n  \"# users\": \"Number of Users\",\n  \"RPO\": \"Recovery Point Objective\",\n  \"RTO\": \"Recovery Time Objective\",\n  \"start date\": \"Go Live Date\",\n  \"end date\": \"End of Support Date\",\n  \"SLA\": \"SLA Type\",\n  \"AGF score\": \"AGF Classification\",\n  \"DR TYpe\": \"DR Type\",\n  \"location\": \"\",\n  \"I/X\": \"User Community\",\n  \"Type of System (Gartner)\": \"Type of System\",\n  \"Across RTA?\": \"Enterprise Level\",\n  \"commercial or customized\": \"Development Type\",\n  \"Cloud Migration plan\": \"Cloud Migration Strategy\",\n  \"Type\": \"Architecture Type\",\n  \"Mobile compatible\": \"Mobile Compliance\",\n  \"support many langs\": \"Multi Language Support\"\n}\n\n# Remove keys with empty string or None values\njson_data = {key: value for key, value in json_data.items() if value != \"\" and value is not None}\njson_data\n","metadata":{"execution":{"iopub.status.busy":"2024-03-28T05:05:41.747070Z","iopub.execute_input":"2024-03-28T05:05:41.748080Z","iopub.status.idle":"2024-03-28T05:05:41.758117Z","shell.execute_reply.started":"2024-03-28T05:05:41.748040Z","shell.execute_reply":"2024-03-28T05:05:41.757071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_excel(UNCLEANED_DATA_PATH)","metadata":{"execution":{"iopub.status.busy":"2024-03-28T08:54:57.751241Z","iopub.execute_input":"2024-03-28T08:54:57.752876Z","iopub.status.idle":"2024-03-28T08:54:57.919410Z","shell.execute_reply.started":"2024-03-28T08:54:57.752826Z","shell.execute_reply":"2024-03-28T08:54:57.917735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Column data types\ndata_types = df.dtypes\n\n# Missing values percentage\nmissing_percentage = (df.isnull().sum() / len(df)) * 100\n\n# Non-missing values count\nnon_missing_count = df.notnull().sum()\n\n# Duplicate values\nduplicate_count = df.duplicated().sum()\n\n# Unique values count\nunique_values_count = df.nunique()\n\n# Most frequent value and its frequency\ntop_frequency = df.mode().iloc[0]\n\n# Create summary DataFrame\nsummary_data = {\n    'Data Types': data_types,\n    'Non-missing Values Count': non_missing_count,\n    'Missing Values (%)': missing_percentage,\n    'Duplicate Values Count': duplicate_count,\n    'Unqiue Values Count': unique_values_count,\n    'Top Frequent Value': top_frequency,\n}\n\nsummary_df = pd.DataFrame(summary_data)\nsummary_df = summary_df.rename_axis('Attribute Names')\nsummary_df","metadata":{"execution":{"iopub.status.busy":"2024-03-28T08:57:34.238891Z","iopub.execute_input":"2024-03-28T08:57:34.239287Z","iopub.status.idle":"2024-03-28T08:57:34.287151Z","shell.execute_reply.started":"2024-03-28T08:57:34.239259Z","shell.execute_reply":"2024-03-28T08:57:34.286029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}