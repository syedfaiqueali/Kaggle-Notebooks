{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install openai==1.14.2","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-03-23T16:20:35.992062Z","iopub.execute_input":"2024-03-23T16:20:35.992416Z","iopub.status.idle":"2024-03-23T16:20:51.045552Z","shell.execute_reply.started":"2024-03-23T16:20:35.992386Z","shell.execute_reply":"2024-03-23T16:20:51.043698Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Requirement already satisfied: openai==1.14.2 in /opt/conda/lib/python3.10/site-packages (1.14.2)\nRequirement already satisfied: anyio<5,>=3.5.0 in /opt/conda/lib/python3.10/site-packages (from openai==1.14.2) (4.2.0)\nRequirement already satisfied: distro<2,>=1.7.0 in /opt/conda/lib/python3.10/site-packages (from openai==1.14.2) (1.9.0)\nRequirement already satisfied: httpx<1,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from openai==1.14.2) (0.27.0)\nRequirement already satisfied: pydantic<3,>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from openai==1.14.2) (2.5.3)\nRequirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from openai==1.14.2) (1.3.0)\nRequirement already satisfied: tqdm>4 in /opt/conda/lib/python3.10/site-packages (from openai==1.14.2) (4.66.1)\nRequirement already satisfied: typing-extensions<5,>=4.7 in /opt/conda/lib/python3.10/site-packages (from openai==1.14.2) (4.9.0)\nRequirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai==1.14.2) (3.6)\nRequirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai==1.14.2) (1.2.0)\nRequirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai==1.14.2) (2024.2.2)\nRequirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai==1.14.2) (1.0.4)\nRequirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai==1.14.2) (0.14.0)\nRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai==1.14.2) (0.6.0)\nRequirement already satisfied: pydantic-core==2.14.6 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai==1.14.2) (2.14.6)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"import io\nimport re\nimport os\nimport json\nimport base64\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n\nfrom openai import OpenAI","metadata":{"execution":{"iopub.status.busy":"2024-03-23T16:20:51.047317Z","iopub.execute_input":"2024-03-23T16:20:51.047676Z","iopub.status.idle":"2024-03-23T16:20:51.054865Z","shell.execute_reply.started":"2024-03-23T16:20:51.047644Z","shell.execute_reply":"2024-03-23T16:20:51.053575Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"# Constant","metadata":{}},{"cell_type":"code","source":"# Env variables\nos.environ[\"OPEN_API_KEY\"] = \"<OPEN-AI-KEY>\"\n\n# File Paths\nCLEANED_DATA_PATH = '/kaggle/input/rta-dubai/sample_clean_application_data_2.xlsx'\nUNCLEANED_DATA_PATH = '/kaggle/input/rta-dubai/sample_unclean_application_data_v2.xlsx'\nMETADATA_PATH = '/kaggle/input/rta-dubai/Application dataset.json'\n\nOPEN_AI_CLIENT = OpenAI(api_key=os.getenv(\"OPEN_API_KEY\") ,organization='org-2EugvoZZidKLd5DkFH3dGTIA')","metadata":{"execution":{"iopub.status.busy":"2024-03-23T16:20:51.056512Z","iopub.execute_input":"2024-03-23T16:20:51.056950Z","iopub.status.idle":"2024-03-23T16:20:51.084338Z","shell.execute_reply.started":"2024-03-23T16:20:51.056911Z","shell.execute_reply":"2024-03-23T16:20:51.083112Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"# Models","metadata":{}},{"cell_type":"code","source":"class MetaData:\n    def __init__(self, data):\n        self.__dict__ = data","metadata":{"execution":{"iopub.status.busy":"2024-03-23T16:20:51.088588Z","iopub.execute_input":"2024-03-23T16:20:51.088941Z","iopub.status.idle":"2024-03-23T16:20:51.094774Z","shell.execute_reply.started":"2024-03-23T16:20:51.088913Z","shell.execute_reply":"2024-03-23T16:20:51.093544Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"# Helper Function","metadata":{}},{"cell_type":"code","source":"def load_json_and_map_to_class(json_path):\n    try:\n        with open(json_path, 'r') as json_file:\n            data = json.load(json_file)\n            return MetaData(data)\n    except json.JSONDecodeError:\n        print(f\"Error: Unable to decode JSON from file '{file_path}'. File may be empty or not in valid JSON format.\")\n        return None\n\n\ndef extract_column_attributes_for_column_name_mapping(metadata_json):\n    columns_meta_data = []\n    for column in metadata_json.attributes:\n        columns_meta_data.append({\n            \"name\": column['attribute_name'],\n            \"description\": column['description'], \n            \"synonyms\": column['synonyms']\n        })\n    return json.dumps(columns_meta_data)\n\n\ndef convert_dataframe_to_html(dataframe):\n    html_string = dataframe.to_html()\n    return html_string.replace('\\n', '')\n\n\ndef extract_python_code(sample_string):\n    # Define a regular expression pattern to match Python code blocks\n    pattern = r'```python\\n(.*?)```'\n\n    # Find all matches of the pattern in the sample string\n    matches = re.findall(pattern, sample_string, re.DOTALL)\n\n    # Return the Python code blocks found\n    return matches\n\n\ndef convert_column_types_and_map_values_wrt_metadata(metadata_json, dataframe):\n    for column in metadata_json.attributes:\n        if column['attribute_name'] in dataframe.columns:\n            if column['data_type'].lower() == \"string\":\n                dataframe[column[\"attribute_name\"]] = dataframe[column[\"attribute_name\"]].astype(str)\n            elif column['data_type'].lower() == \"enum\":\n                # Check if enum value transformation required\n                if column['is_transformation_required']:\n                    dataframe[column[\"attribute_name\"]] = dataframe[column[\"attribute_name\"]].astype(str).str.lower().map(column['values'])\n                dataframe[column[\"attribute_name\"]] = dataframe[column[\"attribute_name\"]].astype('category')\n            elif column['data_type'].lower() == \"date\":\n                dataframe[column[\"attribute_name\"]] = dataframe[column[\"attribute_name\"]].astype('datetime64[ns]')\n                \n    return dataframe\n\n\ndef check_if_any_new_column_formation_is_required(dataframe, metadata_json):\n    new_columns_data = []\n    for column in metadata_json.attributes:\n        if (column['attribute_name'] not in dataframe) and column['constraints']:\n            column_name = column['attribute_name']\n            column_type = column['data_type']\n            column_desc = column['constraints'][0]\n            new_columns_data.append((column_name, column_type, column_desc))\n    return new_columns_data\n\n\ndef extract_df_column_names(dataframe):\n    return \", \".join(dataframe.columns.tolist())\n\n\ndef extract_df_column_names_with_datatypes(dataframe):\n    columns_data_types = {col: str(dtype) for col, dtype in dataframe.dtypes.items()}\n    json_data = json.dumps(columns_data_types)\n    return json_data\n\n\ndef execute_python_script(python_script):\n    slice_script = python_script[9:-3]\n    exec(slice_script)\n    \n\ndef read_saved_dataframe_output():\n    with open('output.txt', 'r') as file:\n        html_content = file.read()\n\n    # Parse HTML content and convert to DataFrame\n    # [TODO] Later remove this, as only for notebook perspective\n    return pd.read_html(html_content)[0]\n\n\ndef read_saved_plot_output():\n    with open('output.png', 'rb') as img_file:\n        img_base64 = base64.b64encode(img_file.read()).decode('utf-8')\n    return img_base64\n\n\ndef read_saved_dataframe_new_col_output():\n    with open('dataframe.txt', 'r') as file:\n        html_content = file.read()\n\n    # Parse HTML content and convert to DataFrame\n    # [TODO] Later remove this, as only for notebook perspective\n    return pd.read_html(html_content)[0]\n\n\ndef contains_visual_keywords(text):\n    visual_keywords = ['plot', 'graph', 'chart', 'visualization']\n    pattern = re.compile(r'\\b(?:' + '|'.join(visual_keywords) + r')\\b', flags=re.IGNORECASE)\n    return True if re.search(pattern, text) else False\n\n\ndef get_html_tag_for_base64_image(img_base64):\n    return f'<img src=\"data:image/png;base64,{img_base64}\" style=\"max-width: 100%; max-height: 100%;\" alt=\"output\">'\n\n\ndef get_dataframe_dtypes_metadata(dataframe):\n    column_types = {col: str(dataframe[col].dtype) for col in df.columns}\n    return json.dumps(column_types)\n\n    \ndef show_saved_output_img():\n    img = mpimg.imread('output.png')\n    img_height, img_width, _ = img.shape\n    aspect_ratio = img_width / img_height\n    fig_width = 10  # Set a default width\n    fig_height = fig_width / aspect_ratio\n    fig = plt.figure(figsize=(fig_width, fig_height))\n    plt.imshow(img, interpolation='nearest')\n    plt.axis('off')\n    plt.tight_layout()\n    # return fig  # Return the figure object","metadata":{"execution":{"iopub.status.busy":"2024-03-23T16:20:51.096852Z","iopub.execute_input":"2024-03-23T16:20:51.097328Z","iopub.status.idle":"2024-03-23T16:20:51.126485Z","shell.execute_reply.started":"2024-03-23T16:20:51.097287Z","shell.execute_reply":"2024-03-23T16:20:51.125516Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"# OpenAI Utils","metadata":{}},{"cell_type":"code","source":"class RTA_Bot:\n    _instance = None\n\n    def __new__(cls, *args, **kwargs):\n        if not cls._instance:\n            cls._instance = super().__new__(cls, *args, **kwargs)\n        return cls._instance\n\n    @staticmethod\n    def ask(prompt):\n        return OPEN_AI_CLIENT.chat.completions.create(\n            model=\"gpt-3.5-turbo\",\n            messages=[\n            {\n                \"role\": \"user\",\n                \"content\": prompt,\n            }],\n        ).choices[0].message.content","metadata":{"execution":{"iopub.status.busy":"2024-03-23T16:20:51.127469Z","iopub.execute_input":"2024-03-23T16:20:51.127777Z","iopub.status.idle":"2024-03-23T16:20:51.138935Z","shell.execute_reply.started":"2024-03-23T16:20:51.127751Z","shell.execute_reply":"2024-03-23T16:20:51.137734Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"def map_column_names_wrt_meta_data(dataset_column_names, metadata_json):\n    prompt = f\"\"\"\nRole: Act as a data analyst whose job is to help in transforming the dataset.\nTask: Consider below defined \"dataset_column_names\". Look up the \"columns_meta_data_json\" and pick relevant column attribute_name w.r.t to the attribute description and synonyms context. Output should be a json with key as 'dataset_column_names' and value as attribute_name picked from columns_meta_data_json\n\ndataset_column_names: {dataset_column_names}\n\ncolumns_meta_data_json: \n{extract_column_attributes_for_column_name_mapping(metadata_json)}\n\nOutput: Return output in JSON Format defined under sample output.\n\nSample Output: {{\"Column1 Old Name\": \"Column1 New Name\", \"Column2 Old Name\": \"Column2 New Name\", ...}}\n\"\"\"\n    response = RTA_Bot.ask(prompt)\n    return json.loads(response)\n\n\ndef ask_rta_bot(question,dataframe):\n    column_names = extract_df_column_names_with_datatypes(dataframe)\n    \n    if contains_visual_keywords(question):\n        execute_visual_prompt(question,column_names)\n    else:\n        output_dataframe = execute_dataframe_prompt(question,column_names)\n        return output_dataframe\n\n\ndef execute_dataframe_prompt(question, column_names):\n    dataframe_name = 'df_uncleaned'\n    prompt = f\"\"\" \nRole: Act as a data analyst whose job is to extract insights from the dataset based on the users asked questions.\n\nTask: Write a valid Python script for the below-mentioned \"user_question\". Use pre-defined / pre-loaded  \"{dataframe_name}\" only for querying the data.\n\nuser_question: {question}\n\nexisting pandas columns names and their types: {column_names}\n\noutput: The output should only be a valid python script\n\nScript steps:\n1. Code for the task\n2. Form a pandas dataframe of the final output\n3. Then convert output dataframe into html string\n4. Save the output into a file named 'output.txt'\n\"\"\"\n    try:\n        # Generate response\n        python_script = RTA_Bot.ask(prompt)\n\n        # Execute script\n        execute_python_script(python_script)\n\n        # Read output dataframe as html table tag\n        output = read_saved_dataframe_output()\n\n        # Remove file\n        # os.remove('output.txt')\n\n        # Return output\n        return output\n    except Exception as e:\n        print(\"I apologize, but I'm currently unable to fulfill your request. Could you please try again or provide me more details to assist me in better understanding your request?\")\n        print(e)\n        return None\n    \n    \ndef execute_visual_prompt(question, column_names):\n    dataframe_name = 'df_uncleaned'\n    prompt = f\"\"\" \nRole: Act as a data analyst whose job is extract insights from the dataset based on the users asked question.\n\nTask: Write a valid python script for the below mentioned \"user_question\". Use pre-defined / pre-loaded {dataframe_name} only for querying the data.\n\nuser_question: {question}\n\nexisting pandas columns names and their types: {column_names}\n\noutput: The output should only be a valid python script\n\nScript steps:\n1. Code for the user's asked question.\n2. Form a plot as the final output but don't write graph plotting code.\n3. Save the output plot as a .png file named 'output.png'\n\"\"\"\n    try:\n        # Generate response\n        python_script = RTA_Bot.ask(prompt)\n\n        # Execute script\n        execute_python_script(python_script)\n\n        # Read output image as html image tag\n        # output = get_html_tag_for_base64_image(read_saved_plot_output())\n        \n        # [REMOVE]\n        # show_saved_output_img()\n        \n        # Remove file\n        # os.remove('output.png')\n\n        # Return output\n        return None # return output\n        \n    except Exception as e:\n        print(\"I apologize, but I'm currently unable to fulfill your request. Could you please try again or provide me more details to assist me in better understanding your request?\")\n        print(e)\n        return None\n\ndef execute_create_new_column_prompt(new_column, existing_df_columns_metadata):\n    new_column_name = new_column[0]\n    new_column_type = new_column[1]\n    new_column_desc = new_column[2]\n    \n    prompt = f\"\"\"\nRole: Act as a data analyst whose job is to create new columns from the below defined 'finalised.csv' data columns names and their data types.\n\nTask: Write a valid python script for the below mentioned \"user_question\". Use 'finalised.csv' only for querying the data.\n\nuser_question: Create a new column named \"{new_column_name}\" with type {new_column_type} and constraints as {new_column_desc}.\n\nexisting pandas columns names and their types: {existing_df_columns_metadata}\n\nScript steps:\n1. Read .csv file named 'finalised.csv'\n2. Convert the data types of the above read 'finalised.csv' dataframe as per the datatypes defined above\n3. Code for the task.\n4. Form a pandas dataframe of the final output with newly added column and its supporting column.\n5. Then convert output dataframe into html string\n6. Save the output into a file named 'dataframe.txt'\n\n    \"\"\"\n    try:\n        # Generate response\n        python_script = RTA_Bot.ask(prompt)\n        print(python_script)\n        \n        # Execute script\n        execute_python_script(python_script)\n\n        # Read output dataframe as html table tag\n        output = read_saved_dataframe_new_col_output()\n\n        # Remove file\n        # os.remove('dataframe.txt')\n\n        # Return output\n        return output\n    except Exception as e:\n        print(\"--------Error in new column formation--------\")\n        print(prompt)\n        print(e)\n        return None\n","metadata":{"execution":{"iopub.status.busy":"2024-03-23T16:24:16.723174Z","iopub.execute_input":"2024-03-23T16:24:16.723609Z","iopub.status.idle":"2024-03-23T16:24:16.741990Z","shell.execute_reply.started":"2024-03-23T16:24:16.723574Z","shell.execute_reply":"2024-03-23T16:24:16.740982Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"# Pipeline\n\n1. Extract **unclean dataset column names** and **attributes objects** from the meta-data.\n2. Use Open-AI to map **column new names** w.r.t to the provided meta-data.\n3. Change columns names.\n4. Transform columns types and their values w.r.t to the **metadata**\n5. Check if new columns formation is left w.r.t to the **metadata**\n6. Talk with the transformed dataset\n\n\n# Code","metadata":{}},{"cell_type":"code","source":"def execute_pipline():\n    # -------------------------------------------------------\n    # 1. Read the un-cleaned dataset and metadata json\n    # -------------------------------------------------------\n    df_uncleaned = pd.read_excel(UNCLEANED_DATA_PATH)\n    metadata_json = load_json_and_map_to_class(METADATA_PATH)\n    \n    \n    # -------------------------------------------------------\n    # 2. Column name mapping w.r.t metadata json\n    # -------------------------------------------------------\n    # Extract old column names\n    uncleaned_column_old_names = ','.join(df_uncleaned.columns)\n\n    # Use Open-Ai to map new column names w.r.t to context\n    uncleaned_column_new_names = map_column_names_wrt_meta_data(uncleaned_column_old_names, metadata_json)\n\n    # Rename columns\n    df_uncleaned.rename(columns=uncleaned_column_new_names, inplace=True)\n\n    \n    # -------------------------------------------------------\n    # 3. Convert column type w.r.t metadata json\n    # -------------------------------------------------------\n    df_uncleaned = convert_column_types_and_map_values_wrt_metadata(metadata_json, df_uncleaned)\n    df_uncleaned.to_csv('finalised.csv', index=False)\n    with open('finalised_metadata.json', \"w\") as json_file:\n        json.dump(get_dataframe_dtypes_metadata(df_uncleaned), json_file)\n    \n\n    # -------------------------------------------------------\n    # 4. Add new column if any missing w.r.t metadatajson\n    # -------------------------------------------------------\n    # Extract new columns data\n#     new_columns = check_if_any_new_column_formation_is_required(df_uncleaned, metadata_json)\n    \n#     # Create new columns\n#     for new_column in new_columns:\n#         new_column_name = new_column[0]\n#         new_column_df = execute_create_new_column_prompt(\n#             new_column,\n#             extract_df_column_names_with_datatypes(df_uncleaned)\n#         )\n#         # Concatenates column to the main dataset & remove duplicates column wise\n#         if new_column_name in new_column_df.columns:\n#             df_uncleaned = pd.concat([df_uncleaned, new_column_df], axis=1)\n#             df_uncleaned = df_uncleaned.loc[:,~df_uncleaned.columns.duplicated()]\n#             df_uncleaned = df_uncleaned.loc[:, ~df_uncleaned.columns.str.contains('^Unnamed')]\n    \n#     # -------------------------------------------------------\n#     # 5. Save modified dataset\n#     # -------------------------------------------------------\n#     df_uncleaned.to_csv('finalised.csv', index=False)\n#     with open('finalised_metadata.json', \"w\") as json_file:\n#         json.dump(get_dataframe_dtypes_metadata(df_uncleaned), json_file)","metadata":{"execution":{"iopub.status.busy":"2024-03-23T16:25:47.675001Z","iopub.execute_input":"2024-03-23T16:25:47.676333Z","iopub.status.idle":"2024-03-23T16:25:47.685507Z","shell.execute_reply.started":"2024-03-23T16:25:47.676289Z","shell.execute_reply":"2024-03-23T16:25:47.684319Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"# Execution","metadata":{}},{"cell_type":"code","source":"%%time\n\ntry:\n    execute_pipline()\n    print(\"Execution successfull\")\nexcept Exception as e:\n    print(\"Execution failed: \", e)","metadata":{"execution":{"iopub.status.busy":"2024-03-23T16:25:56.526533Z","iopub.execute_input":"2024-03-23T16:25:56.526981Z","iopub.status.idle":"2024-03-23T16:26:02.023457Z","shell.execute_reply.started":"2024-03-23T16:25:56.526935Z","shell.execute_reply":"2024-03-23T16:26:02.021993Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"Execution successfull\nCPU times: user 180 ms, sys: 2.62 ms, total: 182 ms\nWall time: 5.49 s\n","output_type":"stream"}]},{"cell_type":"code","source":"df = pd.read_csv('finalised.csv')\ndf.head(2)","metadata":{"execution":{"iopub.status.busy":"2024-03-23T16:26:07.401251Z","iopub.execute_input":"2024-03-23T16:26:07.401654Z","iopub.status.idle":"2024-03-23T16:26:07.428583Z","shell.execute_reply.started":"2024-03-23T16:26:07.401623Z","shell.execute_reply":"2024-03-23T16:26:07.427758Z"},"trusted":true},"execution_count":28,"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"                       Name  Short Name  \\\n0                IBM Maximo  IBM Maximo   \n1  Adobe Experience Manager         AEM   \n\n                                         Description      Business Owner  \\\n0  A comprehensive enterprise asset management (E...  Strategic Planning   \n1  A digital asset management (DAM) system that h...  Strategic Planning   \n\n              Technical Owner Number of Users Recovery Point Objective  \\\n0             Human Resources             415                      NaN   \n1  Logistics and Supply Chain            3000                  60 mins   \n\n  Recovery Time Objective Go Live Date End of Support Date  ...  \\\n0                     NaN   2016-06-30          2023-06-30  ...   \n1          Zero data loss   2022-01-01          2024-11-30  ...   \n\n  System Hosting Place User Community    Type of System Enterprise Level  \\\n0                Cloud           Both  System of Record               No   \n1           On Premise       Internal  System of Record              Yes   \n\n  Development Type Recommendation Cloud Migration Strategy Architecture Type  \\\n0             COTS        Migrate                   Retain         Mainframe   \n1             COTS         Retire                   Retire         Mainframe   \n\n  Mobile Compliance Multi Language Support  \n0     Not Supported                    Yes  \n1     Not Supported                    Yes  \n\n[2 rows x 26 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Name</th>\n      <th>Short Name</th>\n      <th>Description</th>\n      <th>Business Owner</th>\n      <th>Technical Owner</th>\n      <th>Number of Users</th>\n      <th>Recovery Point Objective</th>\n      <th>Recovery Time Objective</th>\n      <th>Go Live Date</th>\n      <th>End of Support Date</th>\n      <th>...</th>\n      <th>System Hosting Place</th>\n      <th>User Community</th>\n      <th>Type of System</th>\n      <th>Enterprise Level</th>\n      <th>Development Type</th>\n      <th>Recommendation</th>\n      <th>Cloud Migration Strategy</th>\n      <th>Architecture Type</th>\n      <th>Mobile Compliance</th>\n      <th>Multi Language Support</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>IBM Maximo</td>\n      <td>IBM Maximo</td>\n      <td>A comprehensive enterprise asset management (E...</td>\n      <td>Strategic Planning</td>\n      <td>Human Resources</td>\n      <td>415</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2016-06-30</td>\n      <td>2023-06-30</td>\n      <td>...</td>\n      <td>Cloud</td>\n      <td>Both</td>\n      <td>System of Record</td>\n      <td>No</td>\n      <td>COTS</td>\n      <td>Migrate</td>\n      <td>Retain</td>\n      <td>Mainframe</td>\n      <td>Not Supported</td>\n      <td>Yes</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Adobe Experience Manager</td>\n      <td>AEM</td>\n      <td>A digital asset management (DAM) system that h...</td>\n      <td>Strategic Planning</td>\n      <td>Logistics and Supply Chain</td>\n      <td>3000</td>\n      <td>60 mins</td>\n      <td>Zero data loss</td>\n      <td>2022-01-01</td>\n      <td>2024-11-30</td>\n      <td>...</td>\n      <td>On Premise</td>\n      <td>Internal</td>\n      <td>System of Record</td>\n      <td>Yes</td>\n      <td>COTS</td>\n      <td>Retire</td>\n      <td>Retire</td>\n      <td>Mainframe</td>\n      <td>Not Supported</td>\n      <td>Yes</td>\n    </tr>\n  </tbody>\n</table>\n<p>2 rows Ã— 26 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"with open('finalised_metadata.json', 'r') as json_file:\n    data = json.load(json_file)\n    print(data)","metadata":{"execution":{"iopub.status.busy":"2024-03-23T16:26:18.368469Z","iopub.execute_input":"2024-03-23T16:26:18.369479Z","iopub.status.idle":"2024-03-23T16:26:18.375205Z","shell.execute_reply.started":"2024-03-23T16:26:18.369441Z","shell.execute_reply":"2024-03-23T16:26:18.373949Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"{\"Name\": \"object\", \"Short Name\": \"object\", \"Description\": \"object\", \"Business Owner\": \"category\", \"Technical Owner\": \"category\", \"Number of Users\": \"object\", \"Recovery Point Objective\": \"object\", \"Recovery Time Objective\": \"object\", \"Go Live Date\": \"datetime64[ns]\", \"End of Support Date\": \"datetime64[ns]\", \"SLA Type\": \"category\", \"Vendor\": \"category\", \"Status\": \"category\", \"AGF Classification\": \"category\", \"DR Type\": \"category\", \"Required Availability\": \"category\", \"System Hosting Place\": \"category\", \"User Community\": \"category\", \"Type of System\": \"category\", \"Enterprise Level\": \"category\", \"Development Type\": \"category\", \"Recommendation\": \"category\", \"Cloud Migration Strategy\": \"category\", \"Architecture Type\": \"category\", \"Mobile Compliance\": \"category\", \"Multi Language Support\": \"category\"}\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}